{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1g5DEWd2dRmg",
        "VFiHFgardZnB",
        "kSYvnBkwdi4q",
        "oFzttGIgd2M4",
        "FKBmg6kOeGZV"
      ],
      "authorship_tag": "ABX9TyNznd4/EkWmIJkHATI+otW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anasFaleh/distributed-ML-Notbook/blob/main/DistributedML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "1g5DEWd2dRmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "from google.colab import drive # Added for Google Drive integration\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, isnull\n",
        "from pyspark.sql.types import IntegerType, LongType, FloatType, DoubleType, BooleanType, DateType, TimestampType, StringType\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
        "from pyspark.ml.regression import LinearRegression, RandomForestRegressor, DecisionTreeRegressor\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator, ClusteringEvaluator\n",
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "v9EEFeN42Jcf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark Session"
      ],
      "metadata": {
        "id": "VFiHFgardZnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global dictionary to store performance metrics\n",
        "performance_metrics = {}\n",
        "\n",
        "try:\n",
        "    spark = (SparkSession.builder\n",
        "             .master(\"local[*]\")\n",
        "             .appName(\"DataProcessor\")\n",
        "             .config(\"spark.driver.memory\", \"2g\")\n",
        "             .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.1\")\n",
        "             .getOrCreate())\n",
        "    print(\"‚úÖ Spark session is ready!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to initialize Spark. Ensure Java is installed correctly. Error: {e}\")\n",
        "    exit()"
      ],
      "metadata": {
        "id": "sQT4bS-G2mVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0b8cf1-98bd-4d67-8e38-6ef94568e143"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Spark session is ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Drive Mounting"
      ],
      "metadata": {
        "id": "kSYvnBkwdi4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_gdrive():\n",
        "    \"\"\"\n",
        "    Mounts Google Drive to the /content/drive directory.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        return \"‚úÖ Google Drive mounted successfully!\"\n",
        "    except Exception as e:\n",
        "        return f\"Failed to mount Google Drive. Error: {e}\"\n",
        "mount_gdrive() # Calling directly to ensure mount before listing files"
      ],
      "metadata": {
        "id": "TXphyHGUdp1s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0f5b5ea7-b9cb-4128-d78d-9616f7dc0530"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'‚úÖ Google Drive mounted successfully!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper & Core Functions"
      ],
      "metadata": {
        "id": "oFzttGIgd2M4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "00bkWM60Lson"
      },
      "outputs": [],
      "source": [
        "def analyze_column_type(df, column_name):\n",
        "    # Analyzes column data type in a Spark DataFrame\n",
        "    try:\n",
        "        dtype = df.schema[column_name].dataType\n",
        "\n",
        "        if isinstance(dtype, (IntegerType, LongType)):\n",
        "            return \"integer\"\n",
        "        if isinstance(dtype, (FloatType, DoubleType)):\n",
        "            return \"numeric\"\n",
        "        if isinstance(dtype, BooleanType):\n",
        "            return \"boolean\"\n",
        "        if isinstance(dtype, (DateType, TimestampType)):\n",
        "            return \"datetime\"\n",
        "\n",
        "        if isinstance(dtype, StringType):\n",
        "            # For strings, infer type from content\n",
        "            sample = df.select(column_name).where(col(column_name).isNotNull()).limit(100).rdd.flatMap(lambda x: x).collect()\n",
        "            if not sample:\n",
        "                return \"empty\"\n",
        "\n",
        "            numeric_count = sum(1 for value in sample if str(value).replace('.', '', 1).isdigit())\n",
        "            numeric_ratio = numeric_count / len(sample)\n",
        "\n",
        "            distinct_count = df.select(column_name).distinct().count()\n",
        "            total_count = df.count()\n",
        "\n",
        "            if numeric_ratio > 0.9:\n",
        "                return \"numeric\"\n",
        "            # This line was problematic as it was unreachable after the above return, removed for clarity if it was intended to infer categorical differently\n",
        "            # elif distinct_count < total_count * 0.1 and distinct_count <= 50: # Arbitrary threshold for categorical\n",
        "            #     return \"categorical\"\n",
        "            else:\n",
        "                return \"text\"\n",
        "\n",
        "        return \"unknown\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing column type for {column_name}: {e}\")\n",
        "        return \"unknown\"\n",
        "\n",
        "def get_file_summary(df, filename, column_info):\n",
        "    # Generates a summary for the uploaded file\n",
        "    n_rows, n_cols = df.count(), len(df.columns)\n",
        "\n",
        "    stats = f\"File Information:\\n- Name: {filename}\\n- Rows: {n_rows:,}\\n- Columns: {n_cols}\\n\\n\"\n",
        "    stats += \"Column Analysis:\\n\"\n",
        "\n",
        "    for info in column_info:\n",
        "        stats += f\"- {info['name']}: `{info['type']}` (Missing: {info['missing_pct']:.1f}%) \\n\"\n",
        "\n",
        "    stats += f\"\\nData Preview (First 5 Rows):\\n\"\n",
        "    preview_df = df.limit(5).toPandas()\n",
        "    stats += preview_df.to_markdown(index=False)\n",
        "\n",
        "    return stats\n",
        "\n",
        "def process_uploaded_file(file_input_or_path):\n",
        "    # Reads an uploaded file, converts to Spark DataFrame, extracts stats\n",
        "    # Accepts Gradio file or string path. Returns summary, df_spark, column_info, file_path\n",
        "    if file_input_or_path is None:\n",
        "        return \"No file uploaded.\", None, None, None\n",
        "\n",
        "    file_path = None\n",
        "    filename = None\n",
        "\n",
        "    if isinstance(file_input_or_path, str):\n",
        "        # Google Drive path or direct path string\n",
        "        file_path = file_input_or_path\n",
        "        filename = os.path.basename(file_path)\n",
        "    elif hasattr(file_input_or_path, 'name'):\n",
        "        # Gradio File object\n",
        "        file_path = file_input_or_path.name\n",
        "        filename = os.path.basename(file_path)\n",
        "    else:\n",
        "        return \"Invalid file input type.\", None, None, None\n",
        "\n",
        "\n",
        "    print(f\"Processing file: {filename} from path: {file_path}\")\n",
        "\n",
        "    df_spark = None\n",
        "    try:\n",
        "        if file_path.startswith('/content/drive/') or file_path.startswith('gs://'): # Check for GDrive or GCS paths\n",
        "            # Use Spark readers for Google Drive paths\n",
        "            if filename.lower().endswith('.csv'):\n",
        "                df_spark = spark.read.option(\"header\", \"true\").csv(file_path)\n",
        "            elif filename.lower().endswith(('.xlsx', '.xls')):\n",
        "                df_spark = spark.read.format(\"com.crealytics.spark.excel\").option(\"header\", \"true\").load(file_path)\n",
        "            else:\n",
        "                return \"Unsupported file type for Google Drive path. Please use CSV or Excel.\", None, None, file_path\n",
        "        else:\n",
        "            # Pandas-based reading for local uploads\n",
        "            if filename.lower().endswith('.csv'):\n",
        "                df_pandas = pd.read_csv(file_path)\n",
        "            elif filename.lower().endswith(('.xlsx', '.xls')):\n",
        "                df_pandas = pd.read_excel(file_path)\n",
        "            else:\n",
        "                return \"Unsupported file type for local upload. Please use CSV or Excel.\", None, None, file_path\n",
        "            df_spark = spark.createDataFrame(df_pandas)\n",
        "\n",
        "        column_info = []\n",
        "        if df_spark.count() > 0: # Check if DataFrame is empty\n",
        "            for col_name in df_spark.columns:\n",
        "                col_type = analyze_column_type(df_spark, col_name)\n",
        "                missing_count = df_spark.filter(col(col_name).isNull()).count()\n",
        "                missing_pct = (missing_count / df_spark.count() * 100) if df_spark.count() > 0 else 0\n",
        "                column_info.append({'name': col_name, 'type': col_type, 'missing_pct': missing_pct})\n",
        "        else:\n",
        "            print(\"Warning: DataFrame is empty. Cannot generate column info.\")\n",
        "\n",
        "\n",
        "        summary_report = get_file_summary(df_spark, filename, column_info)\n",
        "\n",
        "        return summary_report, df_spark, column_info, file_path\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred while processing the file: {str(e)}\", None, None, file_path\n",
        "\n",
        "def save_df_to_gdrive(df, gdrive_path, file_format):\n",
        "    # Saves Spark DataFrame to Google Drive\n",
        "    if df is None:\n",
        "        return \"No DataFrame to save. Load a file first.\"\n",
        "    if not gdrive_path:\n",
        "        return \"Provide a Google Drive path to save the file.\"\n",
        "\n",
        "    try:\n",
        "        if file_format == \"CSV\":\n",
        "            # For CSV, Spark saves as a directory\n",
        "            output_path = gdrive_path.replace('.csv', '') # Remove .csv extension\n",
        "            df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
        "            return f\"DataFrame saved as CSV to '{output_path}' (as a directory of part files).\"\n",
        "        elif file_format == \"Parquet\":\n",
        "            df.write.mode(\"overwrite\").parquet(gdrive_path)\n",
        "            return f\"DataFrame saved as Parquet to '{gdrive_path}'.\"\n",
        "        else:\n",
        "            return \"Unsupported save format. Choose CSV or Parquet.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error saving DataFrame to Google Drive: {str(e)}\"\n",
        "\n",
        "def run_machine_learning_task(file_path_for_ml, task, target_col, num_cores):\n",
        "    # Handles training different machine learning models\n",
        "    global spark\n",
        "    global performance_metrics\n",
        "\n",
        "    if file_path_for_ml is None:\n",
        "        return \"No data file path available for training. Upload a file first.\", \"\"\n",
        "\n",
        "    overall_start_time = time.time()\n",
        "    print(f\"Running ML task '{task}' with {num_cores} cores.\")\n",
        "\n",
        "    result_text = \"\"\n",
        "    performance_report_text = \"\"\n",
        "\n",
        "    try:\n",
        "        # Stop current Spark session\n",
        "        spark.stop()\n",
        "        print(\"Old Spark session stopped.\")\n",
        "\n",
        "        # Re-initialize Spark session with new number of cores\n",
        "        spark = (SparkSession.builder\n",
        "                 .master(f\"local[{num_cores}]\") # Set master to local\n",
        "                 .appName(\"DataProcessor\")\n",
        "                 .config(\"spark.driver.memory\", \"2g\")\n",
        "                 .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:0.13.1\")\n",
        "                 .getOrCreate())\n",
        "        print(f\"New Spark session initialized with {num_cores} cores.\")\n",
        "\n",
        "        # Re-process file to get a new DataFrame\n",
        "        summary, df_new_session, col_info_new, _ = process_uploaded_file(file_path_for_ml)\n",
        "        if df_new_session is None:\n",
        "            return f\"Failed to re-load data for ML task: {summary}\", \"\"\n",
        "\n",
        "        # Use df_new_session and col_info_new for ML tasks\n",
        "        task_specific_start_time = time.time()\n",
        "\n",
        "        if task == \"Clustering (K-Means)\":\n",
        "            result_text = run_kmeans_clustering(df_new_session, col_info_new)\n",
        "        elif task == \"Classification\":\n",
        "            result_text = run_classification_models(df_new_session, target_col, col_info_new)\n",
        "        elif task == \"Regression\":\n",
        "            result_text = run_regression_models(df_new_session, target_col, col_info_new)\n",
        "        else:\n",
        "            result_text = f\"Unknown task: {task}\"\n",
        "\n",
        "        task_specific_end_time = time.time()\n",
        "        task_execution_time = task_specific_end_time - task_specific_start_time\n",
        "        performance_metrics[(task, num_cores)] = task_execution_time\n",
        "        print(f\"Model training for '{task}' completed in {task_execution_time:.2f} seconds with {num_cores} cores.\")\n",
        "        print(f\"Current performance metrics: {performance_metrics}\")\n",
        "\n",
        "        # Generate performance report after the task completes\n",
        "        performance_report_text = generate_performance_report_func(performance_metrics)\n",
        "\n",
        "        return result_text, performance_report_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during model training:\\n{str(e)}\", \"\"\n",
        "    finally:\n",
        "        overall_total_time = time.time() - overall_start_time\n",
        "        print(f\"Total execution time for task (including Spark re-init) '{task}': {overall_total_time:.2f} seconds.\")\n",
        "\n",
        "def run_kmeans_clustering(df, col_info):\n",
        "    # Runs K-Means clustering\n",
        "    result_text = \"## K-Means Clustering (Unsupervised)\\n\\n\"\n",
        "    feature_cols = [c['name'] for c in col_info if c['type'] in (\"numeric\", \"integer\")]\n",
        "\n",
        "    if len(feature_cols) < 2:\n",
        "        return \"K-Means requires at least two numeric/integer columns.\"\n",
        "\n",
        "    result_text += f\"Features Used: {', '.join(feature_cols)}\\n\\n\"\n",
        "\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "    data = assembler.transform(df).select(\"features\")\n",
        "\n",
        "    kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=3, seed=42)\n",
        "    model = kmeans.fit(data)\n",
        "    predictions = model.transform(data)\n",
        "\n",
        "    evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster\")\n",
        "    silhouette = evaluator.evaluate(predictions)\n",
        "\n",
        "    cluster_stats = predictions.groupBy(\"cluster\").count().orderBy(\"cluster\").collect()\n",
        "\n",
        "    result_text += f\"Number of Clusters (k): 3\\n\"\n",
        "    result_text += f\"Silhouette Score: {silhouette:.4f}\\n\\n\"\n",
        "    result_text += \"Cluster Distribution:\\n\"\n",
        "    for row in cluster_stats:\n",
        "        result_text += f\"- Cluster {row['cluster']}: {row['count']:,} samples\\n\"\n",
        "\n",
        "    return result_text\n",
        "\n",
        "def run_classification_models(df, target_col, col_info):\n",
        "    # Runs multiple classification algorithms\n",
        "    if not target_col or target_col == \"No Target\":\n",
        "        return \"A target column must be selected for classification.\"\n",
        "\n",
        "    target_type = next((c['type'] for c in col_info if c['name'] == target_col), None)\n",
        "    if target_type not in (\"categorical\", \"boolean\", \"integer\", \"text\"): # Added 'text' as a potential target for StringIndexer\n",
        "        return f\"Target column '{target_col}' is of type '{target_type}', which is not suitable for classification.\"\n",
        "\n",
        "    feature_cols = [c['name'] for c in col_info if c['type'] in (\"numeric\", \"integer\") and c['name'] != target_col]\n",
        "    if not feature_cols:\n",
        "        return \"No numeric or integer features found for training.\"\n",
        "\n",
        "    indexer = StringIndexer(inputCol=target_col, outputCol=\"label\", handleInvalid=\"skip\")\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "\n",
        "    pipeline = [indexer, assembler]\n",
        "    from pyspark.ml import Pipeline\n",
        "    pipeline_model = Pipeline(stages=pipeline).fit(df)\n",
        "    data = pipeline_model.transform(df).select(\"features\", \"label\")\n",
        "\n",
        "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    result_text = f\"## Classification Algorithms\\n\\n\"\n",
        "    result_text += f\"Target Column: {target_col}\\n\"\n",
        "    result_text += f\"Training Samples: {train_data.count():,}\\n\"\n",
        "    result_text += f\"Test Samples: {test_data.count():,}\\n\\n\"\n",
        "\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(featuresCol=\"features\", labelCol=\"label\"),\n",
        "        \"Decision Tree\": DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\"),\n",
        "        \"Random Forest\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
        "    }\n",
        "\n",
        "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\")\n",
        "\n",
        "    for name, model in models.items():\n",
        "        result_text += f\"### {name}\\n\"\n",
        "        model_start = time.time()\n",
        "        trained_model = model.fit(train_data)\n",
        "        predictions = trained_model.transform(test_data)\n",
        "\n",
        "        accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
        "        f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
        "\n",
        "        result_text += f\"- Training Time: {time.time() - model_start:.2f}s\\n\"\n",
        "        result_text += f\"- Accuracy: {accuracy:.4f}\\n\"\n",
        "        result_text += f\"- F1-Score: {f1:.4f}\\n\\n\"\n",
        "\n",
        "    return result_text\n",
        "\n",
        "def run_regression_models(df, target_col, col_info):\n",
        "    # Runs multiple regression algorithms\n",
        "    if not target_col or target_col == \"No Target\":\n",
        "        return \"A target column must be selected for regression.\"\n",
        "\n",
        "    target_type = next((c['type'] for c in col_info if c['name'] == target_col), None)\n",
        "    if target_type not in (\"numeric\", \"integer\"):\n",
        "        return f\"Target column '{target_col}' is of type '{target_type}', which is not suitable for regression.\"\n",
        "\n",
        "    feature_cols = [c['name'] for c in col_info if c['type'] in (\"numeric\", \"integer\") and c['name'] != target_col]\n",
        "    if not feature_cols:\n",
        "        return \"No numeric or integer features found for training.\"\n",
        "\n",
        "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
        "    data = assembler.transform(df).select(\"features\", target_col)\n",
        "\n",
        "    train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "    result_text = f\"## Regression Algorithms\\n\\n\"\n",
        "    result_text += f\"Target Column: {target_col}\\n\"\n",
        "    result_text += f\"Training Samples: {train_data.count():,}\\n\"\n",
        "    result_text += f\"Test Samples: {test_data.count():,}\\n\\n\"\n",
        "\n",
        "    models = {\n",
        "        \"Linear Regression\": LinearRegression(featuresCol=\"features\", labelCol=target_col),\n",
        "        \"Decision Tree Regressor\": DecisionTreeRegressor(featuresCol=\"features\", labelCol=target_col, maxDepth=5),\n",
        "        \"Random Forest Regressor\": RandomForestRegressor(featuresCol=\"features\", labelCol=target_col, numTrees=10)\n",
        "    }\n",
        "\n",
        "    evaluator = RegressionEvaluator(labelCol=target_col)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        result_text += f\"### {name}\\n\"\n",
        "        model_start = time.time()\n",
        "        trained_model = model.fit(train_data)\n",
        "        predictions = trained_model.transform(test_data)\n",
        "\n",
        "        rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
        "        r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
        "\n",
        "        result_text += f\"- Training Time: {time.time() - model_start:.2f}s\\n\"\n",
        "        result_text += f\"- RMSE (Root Mean Squared Error): {rmse:.4f}\\n\"\n",
        "        result_text += f\"- R Squared: {r2:.4f}\\n\\n\"\n",
        "\n",
        "    return result_text\n",
        "\n",
        "    # Generates table summarizing performance metrics (time, speedup, efficiency)\n",
        "def generate_performance_report_func(current_performance_metrics):\n",
        "    if not current_performance_metrics:\n",
        "        return \"üìä No performance metrics available yet. Please run an ML task first\"\n",
        "\n",
        "    report = \"## üìä Performance Report\\n\\n\"\n",
        "\n",
        "    metrics_by_task = {}\n",
        "    for (task, cores), time_taken in current_performance_metrics.items():\n",
        "        if task not in metrics_by_task:\n",
        "            metrics_by_task[task] = {}\n",
        "        metrics_by_task[task][cores] = time_taken\n",
        "\n",
        "    for task, core_metrics in metrics_by_task.items():\n",
        "        if 1 not in core_metrics:\n",
        "            report += f\"### Task: {task}\\n\"\n",
        "            report += \"No 1-core baseline available to calculate speedup and efficiency for this task.\\n\"\n",
        "            report += \"| Cores | Time (s) |\\n\"\n",
        "            report += \"|-------|----------|\\n\"\n",
        "            sorted_cores = sorted(core_metrics.keys())\n",
        "            for cores in sorted_cores:\n",
        "                time_n_cores = core_metrics[cores]\n",
        "                report += f\"| {cores} | {time_n_cores:.2f} |\\n\"\n",
        "            report += \"\\n\"\n",
        "            continue\n",
        "\n",
        "        time_1_core = core_metrics[1]\n",
        "        report += f\"### Task: {task}\\n\"\n",
        "        report += \"| Cores | Time (s) | Speedup (vs 1 core) | Efficiency |\\n\"\n",
        "        report += \"|-------|----------|---------------------|------------|\\n\"\n",
        "\n",
        "        sorted_cores = sorted(core_metrics.keys())\n",
        "        for cores in sorted_cores:\n",
        "            time_n_cores = core_metrics[cores]\n",
        "            speedup_val = time_1_core / time_n_cores if time_n_cores > 0 else 0.0\n",
        "            efficiency_val = (speedup_val / cores) * 100 if cores > 0 else 0.0\n",
        "\n",
        "            report += f\"| {cores} | {time_n_cores:.2f} | {speedup_val:.2f} | {efficiency_val:.2f}% |\\n\"\n",
        "        report += \"\\n\"\n",
        "\n",
        "    return report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gardio UI"
      ],
      "metadata": {
        "id": "FKBmg6kOeGZV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ef1e2d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "8ca3a81f-2050-4577-f1d5-d4734f19f449"
      },
      "source": [
        "with gr.Blocks(title=\"Distributed ML System\") as app:\n",
        "    gr.Markdown(\"# ‚òÅÔ∏è Distributed Machine Learning System with PySpark\")\n",
        "    gr.Markdown(\"Upload a dataset, then select a machine learning task to train a model and analyze its performance.\")\n",
        "\n",
        "    # State management\n",
        "    df_spark_state = gr.State()\n",
        "    column_info_state = gr.State()\n",
        "    file_path_state = gr.State() # New state to store the file path\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### 1. Upload & Analyze Data\")\n",
        "            gr.Markdown(\"#### From Local/Colab Filesystem\")\n",
        "            file_input = gr.File(label=\"Upload CSV or Excel File\", file_types=[\".csv\", \".xlsx\", \".xls\"])\n",
        "            upload_btn = gr.Button(\"‚úÖ‚ÄÖ Upload & Analyze\", variant=\"primary\")\n",
        "\n",
        "            gr.Markdown(\"#### From Google Drive\")\n",
        "            mount_gdrive_btn = gr.Button(\"Mount Google Drive\", variant=\"secondary\")\n",
        "            gdrive_mount_status = gr.Markdown(\"Google Drive not mounted.\")\n",
        "            gdrive_path_input = gr.Textbox(label=\"Google Drive File Path (e.g., /content/drive/MyDrive/data.csv)\", placeholder=\"Enter path to CSV or Excel file on Google Drive\")\n",
        "            load_gdrive_btn = gr.Button(\"‚úÖ‚ÄÖ Load from Drive & Analyze\", variant=\"secondary\")\n",
        "\n",
        "            gr.Markdown(\"### 2. Configure ML Model\")\n",
        "            target_column_dropdown = gr.Dropdown(label=\"Select Target Column (optional for clustering)\", choices=[], interactive=True)\n",
        "            ml_task_dropdown = gr.Dropdown(\n",
        "                    label=\"Select ML Task\",\n",
        "                    choices=[\"Classification\", \"Regression\", \"Clustering (K-Means)\"],\n",
        "                    value=\"Classification\",\n",
        "                    interactive=True\n",
        "                )\n",
        "            # New dropdown for core selection\n",
        "            num_cores_dropdown = gr.Dropdown(\n",
        "                    label=\"Select Number of Cores\",\n",
        "                    choices=[1, 2, 4, 8],\n",
        "                    value=1,\n",
        "                    interactive=True\n",
        "                )\n",
        "            ml_btn = gr.Button(\"‚ñ∂Ô∏è Train Model\", variant=\"secondary\")\n",
        "\n",
        "            gr.Markdown(\"### 3. Save Processed Data (Optional)\")\n",
        "            save_gdrive_path_input = gr.Textbox(label=\"Save to Google Drive Path (e.g., /content/drive/MyDrive/processed_data)\", placeholder=\"Enter desired output path for processed data\")\n",
        "            save_format_dropdown = gr.Dropdown(label=\"Save Format\", choices=[\"CSV\", \"Parquet\"], value=\"CSV\")\n",
        "            save_gdrive_btn = gr.Button(\"üíæ Save to Drive\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### ‚úÖ‚ÄÖ Analysis & Results\")\n",
        "            analysis_output = gr.Markdown(\"Awaiting file upload...\")\n",
        "            ml_output = gr.Markdown(\"Model training results will appear here...\")\n",
        "            # NEW: Markdown component for performance table\n",
        "            performance_table_output = gr.Markdown('Performance report will appear here...')\n",
        "            save_output = gr.Markdown(\"Save status will appear here...\")\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 5. EVENT HANDLERS\n",
        "    # ==============================================================================\n",
        "\n",
        "    def upload_handler(file):\n",
        "        \"\"\"\n",
        "        Handles the file upload button click event.\n",
        "        \"\"\"\n",
        "        summary, df, col_info, file_path = process_uploaded_file(file)\n",
        "        if df is None:\n",
        "            return summary, None, None, gr.Dropdown(choices=[]), None # Added None for file_path_state\n",
        "\n",
        "        column_names = [c['name'] for c in col_info] if col_info else []\n",
        "        choices_with_none = [\"No Target\"] + column_names\n",
        "\n",
        "        # Update dropdown and return states\n",
        "        return summary, df, col_info, gr.Dropdown(choices=choices_with_none, value=\"No Target\", interactive=True), file_path\n",
        "\n",
        "    # New function to process file from Google Drive path\n",
        "    def process_gdrive_file(gdrive_path):\n",
        "        if not gdrive_path:\n",
        "            return \"‚ùå No Google Drive path provided.\", None, None, gr.Dropdown(choices=[]), None # Added None for file_path_state\n",
        "\n",
        "        # Use the modified process_uploaded_file which accepts a string path\n",
        "        summary, df, col_info, file_path = process_uploaded_file(gdrive_path)\n",
        "        if df is None:\n",
        "            return summary, None, None, gr.Dropdown(choices=[]), file_path # Pass the file_path even if df is None\n",
        "\n",
        "        column_names = [c['name'] for c in col_info] if col_info else []\n",
        "        choices_with_none = [\"No Target\"] + column_names\n",
        "\n",
        "        return summary, df, col_info, gr.Dropdown(choices=choices_with_none, value=\"No Target\", interactive=True), file_path\n",
        "\n",
        "    upload_btn.click(\n",
        "        fn=upload_handler,\n",
        "        inputs=[file_input],\n",
        "        outputs=[analysis_output, df_spark_state, column_info_state, target_column_dropdown, file_path_state] # Added file_path_state\n",
        "    )\n",
        "\n",
        "    mount_gdrive_btn.click(\n",
        "        fn=mount_gdrive,\n",
        "        inputs=[],\n",
        "        outputs=[gdrive_mount_status]\n",
        "    )\n",
        "\n",
        "    load_gdrive_btn.click(\n",
        "        fn=process_gdrive_file,\n",
        "        inputs=[gdrive_path_input],\n",
        "        outputs=[analysis_output, df_spark_state, column_info_state, target_column_dropdown, file_path_state] # Added file_path_state\n",
        "    )\n",
        "\n",
        "    # Modified ml_btn.click to include performance_table_output\n",
        "    ml_btn.click(\n",
        "        fn=run_machine_learning_task,\n",
        "        inputs=[file_path_state, ml_task_dropdown, target_column_dropdown, num_cores_dropdown],\n",
        "        outputs=[ml_output, performance_table_output]\n",
        "    )\n",
        "\n",
        "    save_gdrive_btn.click(\n",
        "        fn=save_df_to_gdrive,\n",
        "        inputs=[df_spark_state, save_gdrive_path_input, save_format_dropdown],\n",
        "        outputs=[save_output]\n",
        "    )\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. APPLICATION LAUNCH\n",
        "# ==============================================================================\n",
        "app.launch(share=True, quiet=True)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Running on public URL: https://e5a1dd8636253f0514.gradio.live\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e5a1dd8636253f0514.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}